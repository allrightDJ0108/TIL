# 📌 Today I Learned : 2024.03.06

<br>

## Today Tasks

- [x]  Elasticsearch Certi Topics 위주로 마지막 검토, 책 개념 같이 보기
- [x]  Elasticsearch Certi 시험 예약
- [ ]  Elasticsearch Gen AI 웨비나
- [ ]  코딩테스트 문제 1개 이상 풀기
- [ ]  취업 공고 조사
- [ ] 자기소개서 1버전 도출


<br>

## Upcoming Tasks

- [ ]  Elasticsearch Certi Topics 위주로 마지막 검토, 책 개념 같이 보기
- [ ]  Elasticsearch Certi 시험 예약
- [ ]  Elasticsearch Gen AI 웨비나
- [ ]  자소서 1차 뽑기
- [ ]  취업 공고 조사
- [ ]  코딩테스트 문제 1개 이상 풀기

<br>

## 새롭게 알게된 내용

### normalizer
keyword 타입의 입력값 대상

### analyzer
text 타입의 입력값 대상
 0개 이상의 캐릭터 필터, 1개의 토크나이저, 0개 이상의 토큰 필터로 구성

- char_filter : 입력한 텍스트를 문자열 변형
- tokenizer : 여러 토큰으로 분리
- filter : 토큰 가공 (추가, 변경, 삭제 등)

#### tokenizer
- standard : 단어 단위 분리 (기본값)
- keyword : 분리하지 않음, 단일 토큰
- whitespace : 공백 단위 분리
- letter : 글지가 아닌 문자 단위로 분리 (공백, 특수문자 등)
- pattern : 지정한 정규식 단위로 구분
- ngram : min_gram 이상 max_gram 이하의 단위로 분리, LIKE 검색 구현 혹은 자동 완성 서비스 구현 시 활용


#### filter
- lowercase : 소문자 변환
- uppercase : 대문자 변환
- stop : 불용어 제거 (the, a, an, in 등)
- synonym : 유의어 내용 치환
- pattern_replace : 정규식으로 내용 치환
- stemmer : 어간 추출 수행 (지원되는 언어 내에서)
- trim : 공백 제거
- truncate : 지정한 길이 단위 분리 


#### 내장 analyzer
- standard : standard + lowercase
- whitespace : whitespace 공백 단위 
- stop : standard + lowercase + stop 
- keyword : keyword 하나의 큰 토큰 반환
- pattern : pattern + lowercase


<br>

## 오늘의 느낀점
```



```
